{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Homework 5\n",
    "#### Student: Liyan Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Illustrating the “curse of dimensionality”\n",
    "#### For a hypersphere of radius $a$ in $d$ dimensions, the volume is related to the surface area of a unit hypersphere ($S$) as\n",
    "\n",
    "$$V=\\dfrac{S \\times a^d}{d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Use this result to show that the fraction of the volume which lies at values of the radius between $a - \\epsilon$ and $a$, where $0 < \\epsilon < a$, is given by $f = 1 - (1 - \\epsilon / a)^d$. Hence, show that for any fixed $\\epsilon$, no matter how small, this fraction tends to $1$ as $d \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is unit hypersphere, we have $V=\\dfrac{S \\times a^d}{d} = 1$.\n",
    "$$\n",
    "\\begin{align}\n",
    "f = V_a - V_{a - \\epsilon} &= \\dfrac{S \\times a^d}{d} - \\dfrac{S \\times (a- \\epsilon)^d}{d}\\\\\n",
    "&= \\dfrac{S}{d} [a^d - (a- \\epsilon)^d]\\\\\n",
    "&= \\dfrac{S \\times a^d}{d} [1 - (1 - \\epsilon / a)^d]\\\\\n",
    "&= 1 - (1 - \\epsilon / a)^d\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For any fixed $\\epsilon$ such that $0 < \\epsilon < a$, we have $0 < 1 - \\epsilon / a < 1$.\n",
    "\n",
    "Therefore, $(1 - \\epsilon / a)^d \\to 0$ when $d \\to \\infty$. Therefore, $f = 1 - (1 - \\epsilon / a)^d \\to 1$ when $d \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Evaluate the ratio $f$ numerically by plotting the results for different values of $\\epsilon / a = 0.01$ and $d = 1, 10, 100$, and $1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot volumn difference\n",
    "from math import pow\n",
    "\n",
    "def calc_f(eps_over_a, d):\n",
    "    return 1 - pow(1 - eps_over_a, d)\n",
    "\n",
    "eps_over_a = 0.01\n",
    "ds = np.logspace(0, 3, 20)\n",
    "fs = [calc_f(eps_over_a, d) for d in ds]\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(ds, fs)\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('Volumn Difference')\n",
    "plt.title('Volumn Difference f when epsilon/a = 0.01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) What conclusions can you draw from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a very small change $\\epsilon$ on radius $a$, the volumn difference $f$ can be very significant when dimension $d$ is large. When $d \\to \\infty$, volumn difference $f$ has the same volumn as the original hypersphere.\n",
    "\n",
    "For randomly distributed data in unit hypersphere with radius $a$, half of the data will lie in the volumn between radius $a$ and $a - \\epsilon$, because $V_a = f = 1$ when $d \\to \\infty$. This means the majority of data will lie around the surface of hypersphere. All data points will be far way from the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA & NMF\n",
    "#### Load the college dataset Colleges.txt provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Preprocess the data by removing missing data and properly dealing with categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1302, 17)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(\"Colleges.txt\", index_col=0, sep='\\t')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apps received</th>\n",
       "      <th>apps accepted</th>\n",
       "      <th>new stud enrolled</th>\n",
       "      <th>% new stud from top 10%</th>\n",
       "      <th>% new stud from top 25%</th>\n",
       "      <th>num FT undergrad</th>\n",
       "      <th>num PT undergrad</th>\n",
       "      <th>in-state tuition</th>\n",
       "      <th>out-of-state tuition</th>\n",
       "      <th>room</th>\n",
       "      <th>board</th>\n",
       "      <th>add fees</th>\n",
       "      <th>est book costs</th>\n",
       "      <th>est personal costs</th>\n",
       "      <th>% fac with PHD</th>\n",
       "      <th>stud:fac ratio</th>\n",
       "      <th>graduation rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>college name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alaska Pacific University</th>\n",
       "      <td>193.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>869.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University of Alaska at Fairbanks</th>\n",
       "      <td>1852.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3885.0</td>\n",
       "      <td>4519.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University of Alaska Southeast</th>\n",
       "      <td>146.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>1849.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>2514.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University of Alaska at Anchorage</th>\n",
       "      <td>2065.0</td>\n",
       "      <td>1598.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6209.0</td>\n",
       "      <td>10537.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>2520.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama Agri. &amp; Mech. Univ.</th>\n",
       "      <td>2817.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3958.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   apps received  apps accepted  \\\n",
       "college name                                                      \n",
       "Alaska Pacific University                  193.0          146.0   \n",
       "University of Alaska at Fairbanks         1852.0         1427.0   \n",
       "University of Alaska Southeast             146.0          117.0   \n",
       "University of Alaska at Anchorage         2065.0         1598.0   \n",
       "Alabama Agri. & Mech. Univ.               2817.0         1920.0   \n",
       "\n",
       "                                   new stud enrolled  % new stud from top 10%  \\\n",
       "college name                                                                    \n",
       "Alaska Pacific University                       55.0                     16.0   \n",
       "University of Alaska at Fairbanks              928.0                      NaN   \n",
       "University of Alaska Southeast                  89.0                      4.0   \n",
       "University of Alaska at Anchorage             1162.0                      NaN   \n",
       "Alabama Agri. & Mech. Univ.                    984.0                      NaN   \n",
       "\n",
       "                                   % new stud from top 25%  num FT undergrad  \\\n",
       "college name                                                                   \n",
       "Alaska Pacific University                             44.0             249.0   \n",
       "University of Alaska at Fairbanks                      NaN            3885.0   \n",
       "University of Alaska Southeast                        24.0             492.0   \n",
       "University of Alaska at Anchorage                      NaN            6209.0   \n",
       "Alabama Agri. & Mech. Univ.                            NaN            3958.0   \n",
       "\n",
       "                                   num PT undergrad  in-state tuition  \\\n",
       "college name                                                            \n",
       "Alaska Pacific University                     869.0            7560.0   \n",
       "University of Alaska at Fairbanks            4519.0            1742.0   \n",
       "University of Alaska Southeast               1849.0            1742.0   \n",
       "University of Alaska at Anchorage           10537.0            1742.0   \n",
       "Alabama Agri. & Mech. Univ.                   305.0            1700.0   \n",
       "\n",
       "                                   out-of-state tuition    room   board  \\\n",
       "college name                                                              \n",
       "Alaska Pacific University                        7560.0  1620.0  2500.0   \n",
       "University of Alaska at Fairbanks                5226.0  1800.0  1790.0   \n",
       "University of Alaska Southeast                   5226.0  2514.0  2250.0   \n",
       "University of Alaska at Anchorage                5226.0  2600.0  2520.0   \n",
       "Alabama Agri. & Mech. Univ.                      3400.0  1108.0  1442.0   \n",
       "\n",
       "                                   add fees  est book costs  \\\n",
       "college name                                                  \n",
       "Alaska Pacific University             130.0           800.0   \n",
       "University of Alaska at Fairbanks     155.0           650.0   \n",
       "University of Alaska Southeast         34.0           500.0   \n",
       "University of Alaska at Anchorage     114.0           580.0   \n",
       "Alabama Agri. & Mech. Univ.           155.0           500.0   \n",
       "\n",
       "                                   est personal costs  % fac with PHD  \\\n",
       "college name                                                            \n",
       "Alaska Pacific University                      1500.0            76.0   \n",
       "University of Alaska at Fairbanks              2304.0            67.0   \n",
       "University of Alaska Southeast                 1162.0            39.0   \n",
       "University of Alaska at Anchorage              1260.0            48.0   \n",
       "Alabama Agri. & Mech. Univ.                     850.0            53.0   \n",
       "\n",
       "                                   stud:fac ratio  graduation rate  \n",
       "college name                                                        \n",
       "Alaska Pacific University                    11.9             15.0  \n",
       "University of Alaska at Fairbanks            10.0              NaN  \n",
       "University of Alaska Southeast                9.5             39.0  \n",
       "University of Alaska at Anchorage            13.7              NaN  \n",
       "Alabama Agri. & Mech. Univ.                  14.3             40.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apps received               10\n",
       "apps accepted               11\n",
       "new stud enrolled            5\n",
       "% new stud from top 10%    235\n",
       "% new stud from top 25%    202\n",
       "num FT undergrad             3\n",
       "num PT undergrad            32\n",
       "in-state tuition            30\n",
       "out-of-state tuition        20\n",
       "room                       321\n",
       "board                      498\n",
       "add fees                   274\n",
       "est book costs              48\n",
       "est personal costs         181\n",
       "% fac with PHD              32\n",
       "stud:fac ratio               2\n",
       "graduation rate             98\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show count of NaN for each column\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with NaN\n",
    "df=df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Use \"college name\" as row identifier. All other features are numerical variables.\n",
    "* Remove rows with any missing values. The resulting set has shape $(471, 17)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Run PCA on this processed data. Report how many components were needed to capture 95% of the variance in the normalized data. Discuss what characterizes the first 3 principal components (i.e., which original features are important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To capture 95% of variance, 3 components are used.\n"
     ]
    }
   ],
   "source": [
    "# Run PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(df.values)\n",
    "print('To capture 95%% of variance, %d components are used.' % pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 14 15  4  3 16 11  9 10 13  2  6  1  0  8  5  7]\n",
      " [15 13 16 14  3  4 12 11 10  9  6  2  1  7  8  5  0]\n",
      " [15  4  3 12 16 14 10 11  9 13  7  2  8  1  6  5  0]]\n"
     ]
    }
   ],
   "source": [
    "# Examine the first 3 principal components\n",
    "feature_idx = np.argsort(np.abs(pca.components_), axis=1)\n",
    "print(feature_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['in-state tuition', 'num FT undergrad', 'out-of-state tuition'], dtype='object')\n",
      "Index(['apps received', 'num FT undergrad', 'out-of-state tuition'], dtype='object')\n",
      "Index(['apps received', 'num FT undergrad', 'num PT undergrad'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print top 3 features in each principal component\n",
    "print(df.columns[feature_idx[0, :-4:-1]])\n",
    "print(df.columns[feature_idx[1, :-4:-1]])\n",
    "print(df.columns[feature_idx[2, :-4:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* To capture 95% of the variance, $3$ components are used.\n",
    "* Top 3 features in the 1st principal component: 'in-state tuition', 'num FT undergrad', 'out-of-state tuition'\n",
    "* Top 3 features in the 2nd principal component: 'apps received', 'num FT undergrad', 'out-of-state tuition'\n",
    "* Top 3 features in the 3rd principal component: 'apps received', 'num FT undergrad', 'num PT undergrad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Normalize the data (where applicable) and run PCA on the normalized data. Report how many components were needed to capture 95% of the variance in the normalized data. Discuss what characterizes the first 3 principal components (i.e., which original features are important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 17)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import normalize\n",
    "college_normalized = normalize(df.values, axis=0)\n",
    "college_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To capture 95% of variance, 8 components are used.\n"
     ]
    }
   ],
   "source": [
    "# Run PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(college_normalized)\n",
    "print('To capture 95%% of variance, %d components are used.' % pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 16  9 12  8  4 15  3 14 13  7 11  6  0  1  2  5]\n",
      " [ 2  5 12 11 14 10  1 15 13  9 16  0  4  8  6  7  3]\n",
      " [ 5 16 13 14 12  2  9  0 15  4  1 10  3  8  7  6 11]]\n",
      "Index(['num FT undergrad', 'new stud enrolled', 'apps accepted'], dtype='object')\n",
      "Index(['% new stud from top 10%', 'in-state tuition', 'num PT undergrad'], dtype='object')\n",
      "Index(['add fees', 'num PT undergrad', 'in-state tuition'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Examine the first 3 principal components\n",
    "feature_idx = np.argsort(np.abs(pca.components_[:3, :]), axis=1)\n",
    "print(feature_idx)\n",
    "print(df.columns[feature_idx[0, :-4:-1]])\n",
    "print(df.columns[feature_idx[1, :-4:-1]])\n",
    "print(df.columns[feature_idx[2, :-4:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* To capture 95% of the variance, $8$ components are used.\n",
    "* Top 3 features in the 1st principal component: 'num FT undergrad', 'new stud enrolled', 'apps accepted'\n",
    "* Top 3 features in the 2nd principal component: '% new stud from top 10%', 'in-state tuition', 'num PT undergrad'\n",
    "* Top 3 features in the 3rd principal component: 'add fees', 'num PT undergrad', 'in-state tuition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Discuss why you should normalize the data before performing PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA tries to maximize the variance along the principal components. Because the scale of data can influence the variance, if data is not normalized, it is likely that PCA will fit the components along the large-scale features, which makes PCA perform badly. Normalization can transform all features to unit scale, which helps to reflect true variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Run NMF on the normalized data using R = 3. Discuss what characterizes the 3 components. How much variance does it capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 17)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NMF\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=3)\n",
    "W = nmf.fit_transform(college_normalized)\n",
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6  2  1  0 11 13 15  3 14 12  4  9 10 16  8  7]\n",
      " [ 6  7 13 12 15 10  8  9 16 14  4  3 11  5  2  1  0]\n",
      " [ 0  7  3  8 11  4 16  1  9 10 14 12  2 15  5 13  6]]\n",
      "Index(['in-state tuition', 'out-of-state tuition', 'graduation rate'], dtype='object')\n",
      "Index(['apps received', 'apps accepted', 'new stud enrolled'], dtype='object')\n",
      "Index(['num PT undergrad', 'est personal costs', 'num FT undergrad'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Examine components\n",
    "feature_idx = np.argsort(nmf.components_, axis=1)\n",
    "print(feature_idx)\n",
    "print(df.columns[feature_idx[0, :-4:-1]])\n",
    "print(df.columns[feature_idx[1, :-4:-1]])\n",
    "print(df.columns[feature_idx[2, :-4:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF with 3 components captures 68.39% variance.\n"
     ]
    }
   ],
   "source": [
    "# Compute variance\n",
    "college_nmf = W @ nmf.components_\n",
    "var_orig = np.sum(np.var(college_normalized, axis=0))\n",
    "var_nmf = np.sum(np.var(college_nmf, axis=0))\n",
    "print('NMF with 3 components captures %.2f%% variance.' % (var_nmf * 100 / var_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Top 3 features in the 3 components are:\n",
    "    * 'in-state tuition', 'out-of-state tuition', 'graduation rate'\n",
    "    * 'apps received', 'apps accepted', 'new stud enrolled'\n",
    "    * 'num PT undergrad', 'est personal costs', 'num FT undergrad'\n",
    "* NMF with 3 components captures $68.39\\%$ variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predicting Loan Defaults with k-Nearest Neighbors\n",
    "#### Consider the Loan Defaults dataset from Homework #3, loan default.csv. You will be using k-NN to predict whether or not a customer will default on a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Preprocess the dataset for k-NN. What did you do and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(\"loan_default.csv\", index_col=0)\n",
    "\n",
    "# Fill out NA on emp_length feature with 5 years\n",
    "df['emp_length'] = df['emp_length'].fillna('5 years')\n",
    "\n",
    "# Encode ordinal features\n",
    "df['term'] = df['term'].astype('category').cat.codes\n",
    "\n",
    "df['grade'] = df['grade'].astype('category').cat.codes\n",
    "\n",
    "df['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
    "df['emp_length'] = df['emp_length'].astype('int64')\n",
    "\n",
    "df['earliest_cr_line'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
    "df['earliest_cr_line'] = df['earliest_cr_line'].astype('int64')\n",
    "\n",
    "dict_cat = {\n",
    "    'verification_status': {\n",
    "        'Not Verified': 0,\n",
    "        'Source Verified': 1,\n",
    "        'Verified': 2\n",
    "    }\n",
    "}\n",
    "df.replace(dict_cat, inplace=True)\n",
    "\n",
    "# Encode non-ordinal features\n",
    "# Merge small and similar categories\n",
    "dict_cat = {\n",
    "    'purpose': {\n",
    "        'wedding': 'vacation',\n",
    "        'renewable_energy': 'other'\n",
    "    }\n",
    "}\n",
    "df.replace(dict_cat, inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"home_ownership\"])\n",
    "df = pd.get_dummies(df, columns=[\"purpose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and label after preparing data\n",
    "y = df['class'].values\n",
    "X = df.drop(columns='class').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to split data into training and test set\n",
    "def split_training_test_set(X, y, test_ratio):\n",
    "    idx_random = np.arange(y.shape[0])\n",
    "    np.random.shuffle(idx_random)\n",
    "    idx_split = int(test_ratio * y.shape[0])\n",
    "\n",
    "    X_test = X[idx_random[:idx_split]]\n",
    "    y_test = y[idx_random[:idx_split]]\n",
    "    X_train = X[idx_random[idx_split:]]\n",
    "    y_train = y[idx_random[idx_split:]]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2280, 37)\n",
      "(570, 37)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "test_ratio = 0.2\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_training_test_set(X, y, test_ratio)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "1. Data are splitted into training and test set in a $8:2$ ratio. The reason for putting 20% of data as test set is that, we can keep a relatively large training set while we still have a descent amount of test samples.\n",
    "2. Categorical features are encoded. The resulting dataset has $37$ features.\n",
    "    1. If the categories are **ordinal**, do **label encoding**.\n",
    "    2. If categories are **non-ordinal**, do **one-hot encoding**.\n",
    "3. Standardization is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Build a k-NN nearest neighbor classifier on the dataset. What was your model assessment and selection strategy (e.g., what were you hyperparameters)? What were the optimal hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5-fold CV, the best n_neighbors is 3, and mean validation f1 score is 0.408\n"
     ]
    }
   ],
   "source": [
    "# Grid search n_neighbors by 5-fold CV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "r_nn = np.arange(3, 15, 2)\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid={'n_neighbors': r_nn}, scoring='f1', cv=5, n_jobs=4)\n",
    "grid.fit(X_train_std, y_train)\n",
    "\n",
    "nn = grid.best_params_['n_neighbors']\n",
    "f1_valid_best = grid.best_score_\n",
    "print('Using 5-fold CV, the best n_neighbors is %d, and mean validation f1 score is %.3f' % (nn, f1_valid_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Select model using 5-fold Cross-Validation. Hyper-parameter is n_neighbors.\n",
    "* Grid search is performed given a range of n_neighbors.\n",
    "* Evaluation metric is F1 score.\n",
    "* The optimal n_neighbors is $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Evaluate the best k-NN model using the hyperparameters from (b). In other words, how well does it do on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on test set is 0.414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=nn)\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "f1_test = f1_score(y_test, knn.predict(X_test_std))\n",
    "print('The F1 score on test set is %.3f' % f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting Loan Defaults with Neural Networks\n",
    "#### Consider the Loan Defaults dataset from Homework #3, loan default.csv. You will be using neural networks to predict whether or not a customer will default on a loan. Note that neural networks can be quite expensive you might want to use a beefier machine to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Preprocess the dataset for neural networks. What did you do and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same preprocessing from Q3:\n",
    "1. Data are splitted into training and test set in a $8:2$ ratio. The reason for putting 20% of data as test set is that, we can keep a relatively large training set while we still have a descent amount of test samples.\n",
    "2. Categorical features are encoded. The resulting dataset has $37$ features.\n",
    "    1. If the categories are **ordinal**, do **label encoding**.\n",
    "    2. If categories are **non-ordinal**, do **one-hot encoding**.\n",
    "3. Standardization is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Build a feedforward neural network on your dataset. How did you select hyperparameters and what were the optimal hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hidden_layer_sizes is 20\n",
      "The best alpha is 0.01000\n",
      "The best learning_rate is 0.001\n",
      "The mean validation f1 score is 0.608\n"
     ]
    }
   ],
   "source": [
    "# Grid search parameters by 5-fold CV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "r_hl = [(5, ), (10,), (20,), (5, 5), (10, 10)]\n",
    "\n",
    "r_alpha = [5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "\n",
    "r_lr = [5e-4, 1e-3, 5e-3, 1e-2]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    MLPClassifier(max_iter=1000),\n",
    "    param_grid={\n",
    "        'hidden_layer_sizes': r_hl,\n",
    "        'alpha': r_alpha,\n",
    "        'learning_rate_init': r_lr\n",
    "    }, scoring='f1', cv=5, n_jobs=4)\n",
    "grid.fit(X_train_std, y_train)\n",
    "\n",
    "hl_best = grid.best_params_['hidden_layer_sizes']\n",
    "alpha_best = grid.best_params_['alpha']\n",
    "lr_best = grid.best_params_['learning_rate_init']\n",
    "f1_valid_best = grid.best_score_\n",
    "\n",
    "print('The best hidden_layer_sizes is %s' % (hl_best,))\n",
    "print('The best alpha is %.5f' % alpha_best)\n",
    "print('The best learning_rate is %.3f' % lr_best)\n",
    "print('The mean validation f1 score is %.3f' % f1_valid_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Select hyper-parameters using 5-fold Cross-Validation.\n",
    "* Grid search is performed.\n",
    "* Evaluation metric is F1 score.\n",
    "* The optimal hidden_layer_sizes is $(20,)$.\n",
    "* The optimal alpha is $0.01$.\n",
    "* The optimal learning_rate is $0.001$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Evaluate the best neural model using the hyperparameters from (b). In other words, how well does it do on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on test set is 0.627\n"
     ]
    }
   ],
   "source": [
    "ffnn = grid.best_estimator_\n",
    "\n",
    "f1_test = f1_score(y_test, ffnn.predict(X_test_std))\n",
    "print('The F1 score on test set is %.3f' % f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Bake-off for Predicting Loan Defaults\n",
    "#### We will consider a new test subsample loan default 2.csv from the Lending Club Loan Data to compare various models and determine the ’best’ model for this dataset. All the models will be evaluated on this new test subsample in terms of AUC, F1, and F2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Select the top 5 features from the dataset (loan default.csv) and build an unregularized logistic regression model. Specify how you select the top 5 features. How well does it perform on the new test subsample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_test = pd.read_csv(\"loan_default_v2.csv\")\n",
    "\n",
    "# Fill out missing values\n",
    "df_test['emp_length'] = df_test['emp_length'].fillna('5 years')\n",
    "\n",
    "# Encode ordinal features\n",
    "df_test['term'] = df_test['term'].astype('category').cat.codes\n",
    "\n",
    "df_test['grade'] = df_test['grade'].astype('category').cat.codes\n",
    "\n",
    "df_test['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
    "df_test['emp_length'] = df_test['emp_length'].astype('int64')\n",
    "\n",
    "df_test['earliest_cr_line'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
    "df_test['earliest_cr_line'] = df_test['earliest_cr_line'].astype('int64')\n",
    "\n",
    "dict_cat = {\n",
    "    'verification_status': {\n",
    "        'Not Verified': 0,\n",
    "        'Source Verified': 1,\n",
    "        'Verified': 2\n",
    "    }\n",
    "}\n",
    "df_test.replace(dict_cat, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                       int64\n",
       "term                             int8\n",
       "int_rate                      float64\n",
       "installment                   float64\n",
       "grade                            int8\n",
       "emp_length                      int64\n",
       "annual_inc                    float64\n",
       "verification_status             int64\n",
       "dti                           float64\n",
       "delinq_2yrs                     int64\n",
       "earliest_cr_line                int64\n",
       "inq_last_6mths                  int64\n",
       "open_acc                        int64\n",
       "pub_rec                         int64\n",
       "revol_bal                       int64\n",
       "revol_util                    float64\n",
       "total_acc                       int64\n",
       "recoveries                    float64\n",
       "collections_12_mths_ex_med      int64\n",
       "acc_now_delinq                  int64\n",
       "tot_coll_amt                    int64\n",
       "tot_cur_bal                     int64\n",
       "total_rev_hi_lim                int64\n",
       "class                           int64\n",
       "home_ownership_MORTGAGE         uint8\n",
       "home_ownership_OTHER            uint8\n",
       "home_ownership_OWN              uint8\n",
       "home_ownership_RENT             uint8\n",
       "purpose_car                     uint8\n",
       "purpose_credit_card             uint8\n",
       "purpose_debt_consolidation      uint8\n",
       "purpose_home_improvement        uint8\n",
       "purpose_house                   uint8\n",
       "purpose_major_purchase          uint8\n",
       "purpose_medical                 uint8\n",
       "purpose_moving                  uint8\n",
       "purpose_other                   uint8\n",
       "purpose_small_business          uint8\n",
       "purpose_vacation                uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode non-ordinal features\n",
    "\n",
    "# Merge small and similar categories\n",
    "dict_cat = {\n",
    "    'purpose': {\n",
    "        'wedding': 'vacation',\n",
    "        'renewable_energy': 'other'\n",
    "    }\n",
    "}\n",
    "df_test.replace(dict_cat, inplace=True)\n",
    "\n",
    "df_test = pd.get_dummies(df_test, columns=[\"home_ownership\"])\n",
    "df_test = pd.get_dummies(df_test, columns=[\"purpose\"])\n",
    "\n",
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that df_test has a new categorical value for home_ownership: home_ownership_OTHER. Add this column to the previous training set at the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                       int64\n",
       "term                             int8\n",
       "int_rate                      float64\n",
       "installment                   float64\n",
       "grade                            int8\n",
       "emp_length                      int64\n",
       "annual_inc                    float64\n",
       "verification_status             int64\n",
       "dti                           float64\n",
       "delinq_2yrs                     int64\n",
       "earliest_cr_line                int64\n",
       "inq_last_6mths                  int64\n",
       "open_acc                        int64\n",
       "pub_rec                         int64\n",
       "revol_bal                       int64\n",
       "revol_util                    float64\n",
       "total_acc                       int64\n",
       "recoveries                    float64\n",
       "collections_12_mths_ex_med      int64\n",
       "acc_now_delinq                  int64\n",
       "tot_coll_amt                    int64\n",
       "tot_cur_bal                     int64\n",
       "total_rev_hi_lim              float64\n",
       "class                           int64\n",
       "home_ownership_MORTGAGE         uint8\n",
       "home_ownership_OTHER            int64\n",
       "home_ownership_OWN              uint8\n",
       "home_ownership_RENT             uint8\n",
       "purpose_car                     uint8\n",
       "purpose_credit_card             uint8\n",
       "purpose_debt_consolidation      uint8\n",
       "purpose_home_improvement        uint8\n",
       "purpose_house                   uint8\n",
       "purpose_major_purchase          uint8\n",
       "purpose_medical                 uint8\n",
       "purpose_moving                  uint8\n",
       "purpose_other                   uint8\n",
       "purpose_small_business          uint8\n",
       "purpose_vacation                uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add home_ownership_OTHER column to training set\n",
    "df['home_ownership_OTHER'] = 0\n",
    "df = pd.concat((df.loc[:, :'home_ownership_MORTGAGE'],\n",
    "                df['home_ownership_OTHER'],\n",
    "                df.loc[:, 'home_ownership_OWN':'purpose_vacation']),\n",
    "               axis=1)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5700, 38)\n",
      "(2850, 38)\n"
     ]
    }
   ],
   "source": [
    "# Get test set\n",
    "y_test = df_test['class'].values\n",
    "X_test = df_test.drop(columns='class').values\n",
    "print(X_test.shape)\n",
    "\n",
    "# Get training set\n",
    "y_train = df['class'].values\n",
    "X_train = df.drop(columns='class').values\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['recoveries', 'int_rate', 'grade', 'annual_inc', 'dti'], dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 features (selected from Q2 Random Forest in HW4)\n",
    "col_top5 = [17, 2, 4, 6, 8]\n",
    "df.columns[col_top5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850, 5)\n",
      "(5700, 5)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset using top 5 features\n",
    "X_train_std_top5 = X_train_std[:, col_top5]\n",
    "X_test_std_top5 = X_test_std[:, col_top5]\n",
    "print(X_train_std_top5.shape)\n",
    "print(X_test_std_top5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to evaluate models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score\n",
    "\n",
    "def report_scores(identifier, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    f1 = f1_score(y_test, y_predicted)\n",
    "    f2 = fbeta_score(y_test, y_predicted, beta=2)\n",
    "    \n",
    "    print('%s: AUC is %.3f, F1 is %.3f, F2 is %.3f' % (identifier, auc, f1, f2))\n",
    "    \n",
    "    return auc, f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression: AUC is 0.839, F1 is 0.662, F2 is 0.556\n"
     ]
    }
   ],
   "source": [
    "# Evaluate logistic regression\n",
    "lgr = LogisticRegression(C=1e8)\n",
    "auc_lgr, f1_lgr, f2_lgr = report_scores('logistic regression', lgr,\n",
    "                                        X_train_std_top5, y_train, X_test_std_top5, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "* Select top 5 features: 'recoveries', 'int_rate', 'grade', 'annual_inc', 'dti'. These are selected from Random Forest feature importance top 5 (Q2 in HW4).\n",
    "* Evaluation on test set: AUC is $0.839$, F1 is $0.662$, F2 is $0.556$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Starting from Homework 3, you’ve used decision trees, random forests, SVM, k-NN and neural networks. For each of these 5 models, re-train the model using the optimal hyperparameters from your homeworks (you should avoid re-tuning to save yourself time) on all of the 2850 training data loan default.csv. How well does it perform on the new test subsample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all five models on the new test set by computing AUC, F1, F2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree: AUC is 0.831, F1 is 0.658, F2 is 0.546\n"
     ]
    }
   ],
   "source": [
    "# Evaluate decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=11, min_samples_leaf=0.089)\n",
    "auc_dtc, f1_dtc, f2_dtc = report_scores('decision tree', dtc, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest: AUC is 0.841, F1 is 0.658, F2 is 0.546\n"
     ]
    }
   ],
   "source": [
    "# Evaluate random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=25, max_depth=20, min_samples_leaf=0.068, max_features=0.6)\n",
    "auc_rfc, f1_rfc, f2_rfc = report_scores('random forest', rfc, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: AUC is 0.752, F1 is 0.575, F2 is 0.582\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', C=600, probability=True)\n",
    "auc_svc, f1_svc, f2_svc = report_scores('SVM', svc, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN: AUC is 0.668, F1 is 0.453, F2 is 0.405\n"
     ]
    }
   ],
   "source": [
    "# Evaluate k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "auc_knn, f1_knn, f2_knn = report_scores('k-NN', knn, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN: AUC is 0.810, F1 is 0.638, F2 is 0.566\n"
     ]
    }
   ],
   "source": [
    "# Evaluate neural network\n",
    "ffnn = MLPClassifier(hidden_layer_sizes=(20,), alpha=0.01, learning_rate_init=0.001, max_iter=1000)\n",
    "auc_ffnn, f1_ffnn, f2_ffnn = report_scores('FFNN', ffnn, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Using the model from (a) as the baseline, how does all the more ’complex’ models compare to the baseline? Compare and contrast the various models with respect to interpretability, computation time, and predictive performance. If you were the loan officer using this model, which one would you use to guide whether or not to give a loan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM</th>\n",
       "      <th>k-NN</th>\n",
       "      <th>FFNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.838973</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>0.841103</td>\n",
       "      <td>0.752481</td>\n",
       "      <td>0.667745</td>\n",
       "      <td>0.810112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.662277</td>\n",
       "      <td>0.658248</td>\n",
       "      <td>0.658248</td>\n",
       "      <td>0.574971</td>\n",
       "      <td>0.452976</td>\n",
       "      <td>0.637691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2</th>\n",
       "      <td>0.556496</td>\n",
       "      <td>0.546241</td>\n",
       "      <td>0.546241</td>\n",
       "      <td>0.581816</td>\n",
       "      <td>0.404963</td>\n",
       "      <td>0.565954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Logistic  Decision Tree  Random Forest       SVM      k-NN      FFNN\n",
       "AUC  0.838973       0.830646       0.841103  0.752481  0.667745  0.810112\n",
       "F1   0.662277       0.658248       0.658248  0.574971  0.452976  0.637691\n",
       "F2   0.556496       0.546241       0.546241  0.581816  0.404963  0.565954"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show table of scores\n",
    "table = pd.DataFrame(\n",
    "    [[auc_lgr, auc_dtc, auc_rfc, auc_svc, auc_knn, auc_ffnn],\n",
    "    [f1_lgr, f1_dtc, f1_rfc, f1_svc, f1_knn, f1_ffnn],\n",
    "    [f2_lgr, f2_dtc, f2_rfc, f2_svc, f2_knn, f2_ffnn]],\n",
    "    ['AUC', 'F1', 'F2'],\n",
    "    ['Logistic', 'Decision Tree', 'Random Forest', 'SVM', 'k-NN', 'FFNN']\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability:\n",
    "1. Decision tree is the easiest to interpret, as it mimics human's decision-making process.\n",
    "2. Random Forest is based on decision tree, and is not hard to interpret. We can look at the splitting at each tree, or get the feature importance across the forest.\n",
    "3. Logistic Regression is not hard to interpret since it is a linear model, although not as interpretable as tree models. We can look at the feature coefficients to get the feature importance (feature needs to be standardized).\n",
    "4. SVM is hard to interpret, especially for non-linear kernels. The prediction is done by using support vectors and  weights are not directly available.\n",
    "5. k-NN is hard to interpret, because the model just memorizes all samples without learning process.\n",
    "6. Neural Network is hard to interpret. There is not easy way to understand what the hidden layers represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation time:\n",
    "1. Decision Tree and Logistic Regression are relatively fast to train and predict.\n",
    "2. Random Forest is slower, but still faster than SVM and Neural Network. It is a bagged learner with many trees, which can be slow; however, it can be computed in parallel to speed up training process.\n",
    "3. SVM and Neural Network are expensive to learn, but cheap to predict.\n",
    "4. k-NN has almost zero training time but expensive to predict, since it needs to go through all samples at prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Performance:\n",
    "1. According to the score table, Logistic Regression, Decision Tree, Random Forest, and Neural Network all have similar performance. Among these four models, Random Forest has the best AUC score; Logistic Regression has the best F1 score; Neural Network has the best F2 score. But overall, there is no huge difference according to performance.\n",
    "2. SVM also has descent performance; it has the highest F2 score overall, though AUC and F1 are lower than the previous four models. AUC and F1 can be potentially improved, since the penalty strength here is selected to only optimize F2 score from HW4.\n",
    "3. k-NN has the worst perforamnce compared to all other models above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were the loan officer, I would use Logistic Regression, and the reasons are as follows:\n",
    "1. The performance is very good based on AUC, F1, F2 scores. It is a linear model which is less likely to overfit.\n",
    "2. It is relatively cheap to train and fast to predict. SVM, Neural Network, k-NN are all more expensive.\n",
    "3. It is interpretable; we can interpret the model by looking at feature coefficients.\n",
    "4. The hyperparameter tuning is easy: we only need to tune the regularization strength. Neural Network, Decision Tree, Random Forest all have more hyperparameters to tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
