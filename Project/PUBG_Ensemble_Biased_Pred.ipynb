{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DfTransformer:\n",
    "    def __init__(self):\n",
    "        self.mean_rankpoints = None\n",
    "        self.mean_killpoints = None\n",
    "        self.mean_winpoints = None\n",
    "        self.df_test_meta = None\n",
    "    \n",
    "    def transform_train(self, df):\n",
    "        # Replace None values of rankPoints with mean\n",
    "        col_rankpoints = df['rankPoints']\n",
    "        col_rankpoints = col_rankpoints.loc[col_rankpoints > 1e-4]\n",
    "        self.mean_rankpoints = col_rankpoints.mean()\n",
    "        # print('The mean of non-none rankPoints is %.3f' % mean_rankpoints)\n",
    "        df.loc[df['rankPoints'] < 1e-4, 'rankPoints'] = self.mean_rankpoints\n",
    "        \n",
    "        # Replace None values of killPoints with mean\n",
    "        col_killpoints = df['killPoints']\n",
    "        col_killpoints = col_killpoints.loc[col_killpoints > 1e-4]\n",
    "        self.mean_killpoints = col_killpoints.mean()\n",
    "        # print('The mean of non-none killPoints is %.3f' % mean_killpoints)\n",
    "        df.loc[df['killPoints'] < 1e-4, 'killPoints'] = self.mean_killpoints\n",
    "        \n",
    "        # Replace None values of winPoints with mean\n",
    "        col_winpoints = df['winPoints']\n",
    "        col_winpoints = col_winpoints.loc[col_winpoints > 1e-4]\n",
    "        self.mean_winpoints = col_winpoints.mean()\n",
    "        # print('The mean of non-none winPoints is %.3f' % mean_winpoints)\n",
    "        df.loc[df['winPoints'] < 1e-4, 'winPoints'] = self.mean_winpoints\n",
    "        \n",
    "        # Feature engineering\n",
    "        return self.feature_engineering(df, is_train=True)\n",
    "    \n",
    "    def transform_test(self, df):\n",
    "        return self.transform_test_directly(df, self.mean_rankpoints,\n",
    "                                            self.mean_killpoints, self.mean_winpoints)\n",
    "    \n",
    "    def transform_test_directly(self, df, mean_rankpoints, mean_killpoints, mean_winpoints):\n",
    "        # Replace None values of rankPoints, killPoints, winPoints\n",
    "        df.loc[df['rankPoints'] < 1e-4, 'rankPoints'] = mean_rankpoints\n",
    "        df.loc[df['killPoints'] < 1e-4, 'killPoints'] = mean_killpoints\n",
    "        df.loc[df['winPoints'] < 1e-4, 'winPoints'] = mean_winpoints\n",
    "        \n",
    "        # Feature engineering\n",
    "        return self.feature_engineering(df, is_train=False)\n",
    "    \n",
    "    def transform_prediction(self, df_meta, y_predicted):\n",
    "        df_y = df_meta.copy()\n",
    "        df_y['winPlacePerc'] = y_predicted\n",
    "        return (self.df_test_meta.merge(df_y, on=['matchId', 'groupId'], how='left'))[['Id', 'winPlacePerc']]\n",
    "    \n",
    "    def feature_engineering(self, df, is_train=True):\n",
    "        # Add hand-engineered features\n",
    "        df_walkDistance = df['walkDistance'] + 5\n",
    "        df['heals_over_dist'] = df['heals'] / df_walkDistance\n",
    "        df['boosts_over_dist'] = df['boosts'] / df_walkDistance\n",
    "        df['kills_over_dist'] = df['kills'] / df_walkDistance\n",
    "        df['headshots_over_dist'] = df['headshotKills'] / df_walkDistance\n",
    "        df['killStreaks_over_dist'] = df['killStreaks'] / df_walkDistance\n",
    "        df['damageDealt_over_dist'] = df['damageDealt'] / df_walkDistance\n",
    "        df['dbnos_over_dist'] = df['DBNOs'] / df_walkDistance\n",
    "        df['weapons_over_dist'] = df['weaponsAcquired'] / df_walkDistance\n",
    "        df['revives_over_dist'] = df['revives'] / df_walkDistance\n",
    "        df_walkDistance = None\n",
    "        df_kills = df['kills'] + 0.001\n",
    "        df['headshots_over_kills'] = df['headshotKills'] / df_kills\n",
    "        df['killStreaks_over_kills'] = df['killStreaks'] / df_kills\n",
    "        df_kills = None\n",
    "        df['teamwork'] = df['assists'] + df['revives']\n",
    "        df['totalDistance'] = df['walkDistance'] + df['rideDistance'] + df['swimDistance']\n",
    "        df['items'] = df['heals'] + df['boosts']\n",
    "        df['skills'] = df['headshotKills'] + df['roadKills']\n",
    "        # df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace'] # No improvement\n",
    "        \n",
    "        features = df.columns.tolist()\n",
    "        features.remove('Id')\n",
    "        features.remove('groupId')\n",
    "        features.remove('matchId')\n",
    "        features.remove('matchType')\n",
    "        if is_train: features.remove('winPlacePerc')\n",
    "            \n",
    "        # Define method to map column names (adding suffix)\n",
    "        def map_col_names(df, features, suffix):\n",
    "            col_name_dict = {}\n",
    "            for name in features:\n",
    "                col_name_dict[name] = name + suffix\n",
    "            return df.rename(columns=col_name_dict)\n",
    "            \n",
    "        # Add group mean and group mean rank in match\n",
    "        group_by = df.groupby(['matchId','groupId'])\n",
    "        group_by_features = group_by[features]\n",
    "        df_agg = group_by_features.agg('mean')\n",
    "        df_agg_rank = df_agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_out = df_agg.reset_index().merge(df_agg_rank, on=['matchId', 'groupId'], how='left',\n",
    "                                            suffixes=[\"_groupMean\", \"_groupMeanRank\"])\n",
    "\n",
    "        # Add group max and group max rank in match\n",
    "        df_agg = group_by_features.agg('max')\n",
    "        df_agg_rank = df_agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_agg = df_agg.reset_index().merge(df_agg_rank, on=['matchId', 'groupId'], how='left',\n",
    "                                            suffixes=[\"_groupMax\", \"_groupMaxRank\"])\n",
    "        df_out = df_out.merge(df_agg, on=['matchId', 'groupId'], how='left')\n",
    "\n",
    "        # Add group min and group min rank in match\n",
    "        df_agg = group_by_features.agg('min')\n",
    "        df_agg_rank = df_agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_agg = df_agg.reset_index().merge(df_agg_rank, on=['matchId', 'groupId'], how='left',\n",
    "                                            suffixes=[\"_groupMin\", \"_groupMinRank\"])\n",
    "        df_out = df_out.merge(df_agg, on=['matchId', 'groupId'], how='left')\n",
    "        \n",
    "        # Add group sum and group sum rank in match\n",
    "        df_agg = group_by_features.agg('sum')\n",
    "        df_agg_rank = df_agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "        df_agg = df_agg.reset_index().merge(df_agg_rank, on=['matchId', 'groupId'], how='left',\n",
    "                                            suffixes=[\"_groupSum\", \"_groupSumRank\"])\n",
    "        df_out = df_out.merge(df_agg, on=['matchId', 'groupId'], how='left')\n",
    "        \n",
    "        # Add group std\n",
    "        df_out = df_out.merge(group_by_features.agg('std').reset_index(), on=['matchId', 'groupId'], how='left')\n",
    "        df_out = df_out.fillna(0)  # zero divisor is present for single player group\n",
    "        df_out = map_col_names(df_out, features, '_groupStd')\n",
    "        group_by_features = None\n",
    "        \n",
    "        # Add group size\n",
    "        df_agg = group_by.size().to_frame('groupSize').reset_index()\n",
    "        df_out = df_out.merge(df_agg, on=['matchId', 'groupId'], how='left')\n",
    "\n",
    "        # Add target if for training\n",
    "        if is_train:\n",
    "            df_agg = group_by[['winPlacePerc']].first().reset_index()\n",
    "            df_out = df_out.merge(df_agg, on=['matchId', 'groupId'], how='left')\n",
    "            \n",
    "        # Add match mean\n",
    "        group_by = df.groupby(['matchId'])\n",
    "        group_by_features = group_by[features]\n",
    "        df_out = df_out.merge(group_by_features.agg('mean').reset_index(), on=['matchId'], how='left')\n",
    "        df_out = map_col_names(df_out, features, '_matchMean')\n",
    "        \n",
    "        # Add match max\n",
    "        df_out = df_out.merge(group_by_features.agg('max').reset_index(), on=['matchId'], how='left')\n",
    "        df_out = map_col_names(df_out, features, '_matchMax')\n",
    "        \n",
    "        # Add match min\n",
    "        df_out = df_out.merge(group_by_features.agg('min').reset_index(), on=['matchId'], how='left')\n",
    "        df_out = map_col_names(df_out, features, '_matchMin')\n",
    "        \n",
    "        # Add match sum\n",
    "        df_out = df_out.merge(group_by_features.agg('sum').reset_index(), on=['matchId'], how='left')\n",
    "        df_out = map_col_names(df_out, features, '_matchSum')\n",
    "        \n",
    "        # Add match std\n",
    "        df_out = df_out.merge(group_by_features.agg('std').reset_index(), on=['matchId'], how='left')\n",
    "        df_out = df_out.fillna(0)\n",
    "        df_out = map_col_names(df_out, features, '_matchStd')\n",
    "        group_by_features = None\n",
    "        \n",
    "        # Add match size\n",
    "        df_agg = group_by['groupId'].nunique().to_frame('matchSize').reset_index()\n",
    "        df_out = df_out.merge(df_agg, on=['matchId'], how='left')\n",
    "\n",
    "        # Add encoded matchType (no improvement)\n",
    "        # df_agg = group_by['matchType'].first().reset_index()\n",
    "        # df_out = df_out.merge(df_agg, on=['matchId'], how='left')\n",
    "        # df_out = pd.get_dummies(df_out, columns=['matchType']) # Ont-hot encoding\n",
    "        # df_out['matchType'] = df_out['matchType'].astype('category').cat.codes # Label encoding\n",
    "        \n",
    "        # Keep metadata of df_test for later restoring individual prediction\n",
    "        if not is_train:\n",
    "            self.df_test_meta = df[['Id', 'matchId', 'groupId']]\n",
    "        \n",
    "        # Return: features, metadata, weights\n",
    "        return df_out.drop(columns=['matchId', 'groupId']),\\\n",
    "               df_out[['matchId', 'groupId']],\\\n",
    "               df_out['groupSize'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2026744, 548)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test data\n",
    "df_test = pd.read_csv('test_V2.csv', nrows=None)\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df_transformer = DfTransformer()\n",
    "mean_rankpoints = 1494.34089\n",
    "mean_killpoints = 1253.6821744\n",
    "mean_winpoints = 1505.542888\n",
    "\n",
    "df_test, df_test_meta, _ = df_transformer.transform_test_directly(\n",
    "    df_test, mean_rankpoints, mean_killpoints, mean_winpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X_test = df_test.values\n",
    "df_test = None\n",
    "print(X_test.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_scaler = pd.read_csv('training_scaler.csv')\n",
    "print(df_scaler.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.scale_ = df_scaler['scale'].values\n",
    "scaler.mean_ = df_scaler['mean'].values\n",
    "scaler.var_ = df_scaler['var'].values\n",
    "df_scaler = None\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "from joblib import load\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "from keras.models import load_model\n",
    "\n",
    "lr = load('LR.joblib')\n",
    "lgb = lgb.Booster(model_file='LightGBM_Model.txt')\n",
    "nn = load_model('NN_Model.h5')\n",
    "\n",
    "stacking_model = lgb.Booster(model_file='Stacking_Model.txt')\n",
    "\n",
    "models = [lr, lgb, nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to build new dataset for stacking-level model\n",
    "def build_stacking_dataset(models, X, df_meta, is_train=True):\n",
    "    # Load and preprocess original training set\n",
    "    df = None\n",
    "    if is_train:\n",
    "        df = pd.read_csv('train_V2.csv')\n",
    "    else:\n",
    "        df = pd.read_csv('test_V2.csv')\n",
    "    df = df.dropna()\n",
    "    # Fill out None values\n",
    "    mean_rankpoints = 1494.34089\n",
    "    mean_killpoints = 1253.6821744\n",
    "    mean_winpoints = 1505.542888\n",
    "    df.loc[df['rankPoints'] < 1e-4, 'rankPoints'] = mean_rankpoints\n",
    "    df.loc[df['killPoints'] < 1e-4, 'killPoints'] = mean_killpoints\n",
    "    df.loc[df['winPoints'] < 1e-4, 'winPoints'] = mean_winpoints\n",
    "    \n",
    "    # Add predictions to original training set\n",
    "    for idx, model in enumerate(models):\n",
    "        df_intermediate = df_meta.copy()\n",
    "        df_intermediate['pred_' + idx] = model.predict(X)\n",
    "        df = df.merge(df_intermediate, on=['matchId', 'groupId'], how='left')\n",
    "    \n",
    "    if is_train:\n",
    "        y = df['winPlacePerc'].values\n",
    "        df = df.drop(columns=['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc'])\n",
    "        return df.values, df.columns, y\n",
    "    else:\n",
    "        ids = df['Id']\n",
    "        df = df.drop(columns=['Id', 'groupId', 'matchId', 'matchType'])\n",
    "        return df.values, df.columns, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stacking dataset\n",
    "X_test, feature_name, ids = build_stacking_dataset(models, X_test, df_test_meta, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction\n",
    "df_predicted = pd.DataFrame()\n",
    "df_predicted['Id'] = ids\n",
    "df_predicted['winPlacePerc'] = stacking_model.predict(X_test)\n",
    "df_predicted.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to plot feature importance\n",
    "def plot_feature_importance(feature_importance, feature_name):\n",
    "    idx_sorted = np.argsort(feature_importance)\n",
    "    importance_sorted = feature_importance[idx_sorted]\n",
    "    name_sorted = [feature_name[i] for i in idx_sorted]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.barh(np.arange(feature_importance.size), importance_sorted, tick_label=name_sorted)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "topK = 20\n",
    "feature_importance = stacking_model.feature_importance() # Number of splits\n",
    "idx_sorted = np.argsort(feature_importance)\n",
    "idx_topK = idx_sorted[-1:-topK-1:-1]\n",
    "\n",
    "plot_feature_importance(feature_importance[idx_topK], feature_name[idx_topK].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
