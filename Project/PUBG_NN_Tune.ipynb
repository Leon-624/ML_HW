{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_output = 'tune_nn.output'\n",
    "with open(filename_output, 'a') as f:\n",
    "    f.write('%s starts\\n' % ('PUBG_NN_Tune'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug: load processed data from saved file directly\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train_meta = pd.read_csv('df_train_meta.csv')\n",
    "df_train_weight = pd.read_csv('df_train_weight.csv')\n",
    "weight_train = df_train_weight['weight_train'].values\n",
    "df_train_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2026744, 548)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get X and y\n",
    "y_train = df_train['winPlacePerc'].values\n",
    "X_train = df_train.drop(columns='winPlacePerc').values\n",
    "\n",
    "feature_name = df_train.columns\n",
    "df_train = None\n",
    "\n",
    "print(X_train.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Define method to build model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def build_model(input_dim, hidden_layers, reg_strength, dropout_rate, lr):\n",
    "    model = Sequential()\n",
    "    \n",
    "    for idx, size in enumerate(hidden_layers):\n",
    "        regularizer = None if reg_strength < 1e-6 else l2(reg_strength)\n",
    "        if idx == 0:\n",
    "            model.add(Dense(size, activation='relu', kernel_regularizer=regularizer, input_dim=input_dim))\n",
    "        else:\n",
    "            model.add(Dense(size, activation='relu', kernel_regularizer=regularizer))\n",
    "        if dropout_rate > 1e-6: model.add(Dropout(dropout_rate))\n",
    "            \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    optimizer = optimizers.Adam(lr)\n",
    "    model.compile(optimizer, loss='mse', weighted_metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to fit model with callbacks\n",
    "def fit_model(model, X, y, weight, epochs=100, early_stop_patience=None, lr=None, decay_factor=None, step_size=None):\n",
    "    callbacks = []\n",
    "    if early_stop_patience is not None:\n",
    "        callbacks.append(EarlyStopping(monitor='val_weighted_mean_absolute_error', min_delta=1e-5,\n",
    "                                       patience=early_stop_patience, restore_best_weights=True))\n",
    "    if decay_factor is not None and step_size is not None:\n",
    "        callbacks.append(LearningRateScheduler(\n",
    "            lambda epoch, curr_lr: lr * (decay_factor ** np.floor(epoch/step_size)),\n",
    "            verbose=0))\n",
    "    \n",
    "    return model.fit(X, y, sample_weight=weight, callbacks=callbacks,\n",
    "                     batch_size=20000, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method to search parameters by holdout\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def search_nn_params(X, y, weight, l_hidden_layers, l_reg_strength,\n",
    "                     l_dropout_rate, l_lr, epochs, holdout_itr=1, holdout_ratio=0.2):\n",
    "    best_mae_valid = None\n",
    "    best_hidden_layers = None\n",
    "    best_reg_strength = None\n",
    "    best_dropout_rate = None\n",
    "    best_lr = None\n",
    "    \n",
    "    for hidden_layers in l_hidden_layers:\n",
    "        for reg_strength in l_reg_strength:\n",
    "            for dropout_rate in l_dropout_rate:\n",
    "                for lr in l_lr:\n",
    "                    maes = []\n",
    "                    ss = ShuffleSplit(n_splits=holdout_itr, test_size=holdout_ratio)\n",
    "                    for idx_train, idx_valid in ss.split(X):\n",
    "                        X_train = X[idx_train]\n",
    "                        y_train = y[idx_train]\n",
    "                        X_valid = X[idx_valid]\n",
    "                        y_valid = y[idx_valid]\n",
    "                        weight_train = weight[idx_train]\n",
    "                        weight_valid = weight[idx_valid]\n",
    "                        \n",
    "                        model = build_model(X_train.shape[1], hidden_layers, reg_strength, dropout_rate, lr)\n",
    "                        fit_model(model, X_train, y_train, weight_train, epochs=epochs,\n",
    "                                  early_stop_patience=None, lr=lr, decay_factor=0.7, step_size=10)\n",
    "                        \n",
    "                        maes.append(mean_absolute_error(y_valid, model.predict(X_valid), sample_weight=weight_valid))\n",
    "                    \n",
    "                    mae = np.array(maes).mean()\n",
    "                    \n",
    "                    with open(filename_output, 'a') as f:\n",
    "                        f.write('hidden_layers = %s, reg_strength = %.4f, dropout_rate = %.3f, lr = %.4f, MAE = %.4f\\n'\n",
    "                                % (hidden_layers, reg_strength, dropout_rate, lr, mae))\n",
    "                    print('hidden_layers = %s, reg_strength = %.3f, dropout_rate = %.3f, lr = %.4f, MAE = %.4f'\n",
    "                         % (hidden_layers, reg_strength, dropout_rate, lr, mae))\n",
    "                    \n",
    "                    if best_mae_valid is None or mae < best_mae_valid:\n",
    "                        best_mae_valid = mae\n",
    "                        best_hidden_layers = hidden_layers\n",
    "                        best_reg_strength = reg_strength\n",
    "                        best_dropout_rate = dropout_rate\n",
    "                        best_lr = lr\n",
    "\n",
    "    return best_hidden_layers, best_reg_strength, best_dropout_rate, best_lr, best_mae_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layers = [512, 256, 128, 64, 32], reg_strength = 0.000, dropout_rate = 0.000, lr = 0.0010, MAE = 0.0305\n"
     ]
    }
   ],
   "source": [
    "# Search params\n",
    "l_hidden_layers = [[512, 256, 128, 64, 32], [512, 256, 128, 64], [256, 128, 64, 32], [512, 256, 128]]\n",
    "l_reg_strength = [0, 0.0005, 0.001]\n",
    "l_dropout_rate = [0, 0.05]\n",
    "l_lr = [0.001]\n",
    "\n",
    "params = search_nn_params(X_train, y_train, weight_train, l_hidden_layers, l_reg_strength,\n",
    "                          l_dropout_rate, l_lr, epochs=20, holdout_itr=1, holdout_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_output, 'a') as f:\n",
    "    f.write('best parameters: hidden_layers = %s, reg_strength = %.4f, dropout_rate = %.3f, lr = %.4f, MAE = %.4f\\n'\n",
    "            % (params[0], params[1], params[2], params[3], params[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
