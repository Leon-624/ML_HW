{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug: load processed data from saved file directly\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_train_meta = pd.read_csv('df_train_meta.csv')\n",
    "df_train_weight = pd.read_csv('df_train_weight.csv')\n",
    "weight_train = df_train_weight['weight_train'].values\n",
    "df_train_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2026744, 548)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get X and y\n",
    "y_train = df_train['winPlacePerc'].values\n",
    "X_train = df_train.drop(columns='winPlacePerc').values\n",
    "\n",
    "feature_name = df_train.columns\n",
    "df_train = None\n",
    "\n",
    "print(X_train.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "from joblib import load\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "from keras.models import load_model\n",
    "\n",
    "lr = load('LR.joblib')\n",
    "lg = lgb.Booster(model_file='LightGBM_Model.txt')\n",
    "nn = load_model('NN_Model.h5')\n",
    "\n",
    "models = [lr, lg, nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method to build new dataset for stacking-level model\n",
    "def build_stacking_dataset(models, X, df_meta, is_train=True):\n",
    "    # Load and preprocess original training set\n",
    "    df = None\n",
    "    if is_train:\n",
    "        df = pd.read_csv('train_V2.csv')\n",
    "    else:\n",
    "        df = pd.read_csv('test_V2.csv')\n",
    "    df = df.dropna()\n",
    "    # Fill out None values\n",
    "    mean_rankpoints = 1494.34089\n",
    "    mean_killpoints = 1253.6821744\n",
    "    mean_winpoints = 1505.542888\n",
    "    df.loc[df['rankPoints'] < 1e-4, 'rankPoints'] = mean_rankpoints\n",
    "    df.loc[df['killPoints'] < 1e-4, 'killPoints'] = mean_killpoints\n",
    "    df.loc[df['winPoints'] < 1e-4, 'winPoints'] = mean_winpoints\n",
    "    \n",
    "    # Add predictions to original training set\n",
    "    for idx, model in enumerate(models):\n",
    "        df_intermediate = df_meta.copy()\n",
    "        df_intermediate['pred_' + str(idx)] = model.predict(X)\n",
    "        df = df.merge(df_intermediate, on=['matchId', 'groupId'], how='left')\n",
    "    \n",
    "    if is_train:\n",
    "        y = df['winPlacePerc'].values\n",
    "        df = df.drop(columns=['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc'])\n",
    "        return df.values, df.columns, y\n",
    "    else:\n",
    "        ids = df['Id']\n",
    "        df = df.drop(columns=['Id', 'groupId', 'matchId', 'matchType'])\n",
    "        return df.values, df.columns, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stacking dataset\n",
    "X_train, feature_name, y_train = build_stacking_dataset(models, X_train, df_train_meta, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10671"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare validation set\n",
    "ratio_valid = 0.05\n",
    "\n",
    "idx_shuffle = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx_shuffle)\n",
    "idx_split = int(X_train.shape[0] * ratio_valid)\n",
    "idx_valid = idx_shuffle[:idx_split]\n",
    "idx_train = idx_shuffle[idx_split:]\n",
    "\n",
    "X_valid = X_train[idx_valid]\n",
    "y_valid = y_train[idx_valid]\n",
    "lgb_data_valid = lgb.Dataset(X_valid, label=y_valid, free_raw_data=True)\n",
    "\n",
    "X_train = X_train[idx_train]\n",
    "y_train = y_train[idx_train]\n",
    "lgb_data_train = lgb.Dataset(X_train, label=y_train, free_raw_data=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxu85/playground/lib/python3.5/site-packages/lightgbm/engine.py:116: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/lxu85/playground/lib/python3.5/site-packages/lightgbm/engine.py:121: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\ttraining's l1: 0.016868\tvalid_1's l1: 0.0169399\n",
      "[400]\ttraining's l1: 0.0166869\tvalid_1's l1: 0.0167757\n",
      "[600]\ttraining's l1: 0.0165951\tvalid_1's l1: 0.0167043\n",
      "[800]\ttraining's l1: 0.0165285\tvalid_1's l1: 0.0166591\n",
      "[1000]\ttraining's l1: 0.0164775\tvalid_1's l1: 0.0166294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's l1: 0.0164775\tvalid_1's l1: 0.0166294\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM on stacking dataset\n",
    "\n",
    "# Define model parameters\n",
    "lgb_params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':1000, 'early_stopping_rounds':50,\n",
    "              \"num_leaves\" : 31, \"learning_rate\" : 0.1, \"bagging_fraction\" : 0.7,\n",
    "               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
    "             }\n",
    "\n",
    "# Train model\n",
    "model = lgb.train(lgb_params, lgb_data_train,\n",
    "                  valid_sets=[lgb_data_train, lgb_data_valid],\n",
    "                  verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f72d8263e48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "model.save_model('Stacking_Model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
